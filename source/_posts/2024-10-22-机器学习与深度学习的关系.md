---
title: 机器学习与深度学习的关系
tags:
  - machine learning
  - deep learning
date: 2024-10-22 01:26:26
---

![ai-vs-machine-learning-vs-deep-learning](/images/post/ml/ai-vs-machine-learning-vs-deep-learning.png)

<center><font size="2">from: <a href="https://learn.microsoft.com/zh-cn/azure/machine-learning/concept-deep-learning-vs-machine-learning?view=azureml-api-2">Azure 机器学习中的深度学习与机器学习</a></font></center>

[**机器学习（ML，machine learning）**](https://en.wikipedia.org/wiki/Machine_learning)是[**数据科学（data science）**](https://en.wikipedia.org/wiki/Data_science)和[**人工智能（AI，artificial intelligence）**](https://en.wikipedia.org/wiki/Artificial_intelligence)的一个**子集（subset）**，也是实现人工智能的途径之一，机器学习并不是某种具体的算法，而是存在非常多的实现方式、框架、算法，具有多种不同的[**学习范式（learning paradigms）**](https://en.wikipedia.org/wiki/Machine_learning#Approaches)，其在现代具有非常广泛的应用且综合性非常强，是一门跨越多领域、多学科的[**科际整合 / 跨学科（interdisciplinarity）**](https://en.wikipedia.org/wiki/Interdisciplinarity)。

## 标签、特征与模型

**标签（label）**、**特征（feature）**和**模型（model）**是在机器学习中普遍存在的三个基础概念，同时也是核心概念。

**标签**为**[预测（prediction）](https://en.wikipedia.org/wiki/Prediction#Statistics)**所得到的**目标（target）**值，这里以[**监督学习（SL，supervised learning）**](https://en.wikipedia.org/wiki/Supervised_learning)常见的**分类任务（classification）**和**回归任务（regression）**来看，它们所得到的分别是**定性（qualitative）**输出和**定量（quantitative）**输出。对于分类问题，标签可能是某些**离散（discrete）**的**类别**，也就是对输入进行**定性**，比如说，使用某个模型对图片进行物体分类检测，并预测出图片中的物体是一只“小狗”。对于回归问题，标签可能是某些**连续（continuous）**的**数值**，可理解为在某一范围内任意的一个**确切值**，比如说，根据最近的天气情况，来预测明天的“气温”。以上的小狗和气温，分别就是对应的分类任务和回归任务根据输入所预测出的目标（标签）。

**特征**是从原始数据中通过**特征工程**所提取出来的，为数据自身的固有**属性**，例如：某个人的年纪、性别、出生地等信息；某字体文字图像根据特定算法计算出某文字的特征值等。数据中可能包含着大量的特征信息（如例子中的个人信息就包含了多个特征值），现实数量可能成百上千，甚至百万以上都有可能，这些特征组成了数据的**特征集（feature set）**，每个特征可使用**权重（weight）**来表明其对模型输出结构的贡献程度，或者说特征的重要程度。如果特征数量太多，有些特征可能是不必要的，通常会使用**特征选择**来对原始特征集进行精简，得到一个**特征子集（feature subset）**。

标签和特征可以是任何东西，也可以有任意数量，并没有什么硬性的规定，而是根据实际需求而定。

**模型**是通过机器学习**训练**出来的，特征和标签是构建这些模型的基础，而模型定义了**特征**与**标签**之间的**映射关系（mapping）**，根据要解决的问题的不同，模型的种类有很多。模型通常都具有**泛化能力（generalization）**，即具有能够对未知（新）数据进行有效**预测**的能力，对于多数简单的情况，在实际应用模型时，通常可以将**特征**视为模型的**输入**，而将**标签**视为模型的**输出**。

## 数据集

机器学习中的**数据集（dataset）**也可称为**样本集（sample set）**，如果以任意方式对数据集进行观测，可将每一次观测到的任意内容定义为一个**数据点（data point）**，这在统计学中也称为**观测值（observation）**，数据点的定义非常灵活，假设每个**样本（sample）**为一行，而每行样本中有多列信息（如特征），那么一个数据点可以是一个样本的整行数据，也可以是样本部分列的组合。机器学习的**通常做法**是将数据集中的**每个样本**视为一个**数据点**，而每个数据点为一个**基本单位**，因此，样本和数据点这两个概念一般可以互换使用。

数据集可以是**结构化数据集（structured dataset）**，其每个样本可包含多个特征和标签，不包含标签的样本是**无标签数据（unlabeled data）**，携带标签的则是[**有标签数据（labeled data）**](https://en.wikipedia.org/wiki/Labeled_data)，这两种数据适合于不同的情景。一般可根据实际情况，在开发机器学习模型的**预处理阶段（preprocessing stage）**，执行**数据标记（data labeling）**或**数据注释（data annotation）**处理，即识别原始数据，并为数据添加一个或多个标签，以为模型指定其上下文，从而使机器学习模型做出准确的预测。

数据集也可以是**非结构化数据集（unstructured dataset）**，比如在深度学习中，通常更适合使用图像、音频文件、视频文件等类型的数据，不过这通常需要高度的抽象化才能够提取特征。

数据集中的样本数据，根据实际情况，可能需要进行不同程度的**人为干预**，添加一些额外信息（如标签）以帮助模型更好地（或者说更准确地）理解数据。

数据集理论上是越大越好，为了获得更稳定的结果和对未知数据上预测能力的精度评估，通常会采用**[交叉验证（CV，cross validation）](https://en.wikipedia.org/wiki/Cross-validation_(statistics))**的做法，最基本的方式是 [**k 折交叉验证（k-fold cross validation）**](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation)。将整个数据集拆分为多个子集，这种拆分是有必要的，最起码需要分出两种，一般可分出三种数据集：**训练集（training dataset）**、**验证集（validation dataset）**和**测试集（test dataset）**，简单来说，训练集用于拟合模型（或者说训练模型），验证集对模型的泛化能力进行评估，同时调整超参数，使模型处于最好的状态，测试集则是对调整后的模型进行最终评估，详细可看：[Training, validation, and test data sets](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets)。

## 基本工作方式

> [**基准真相 / 真实值（ground-truth）**](https://en.wikipedia.org/wiki/Ground_truth)：通过直接观测或测量得到的真实信息
>
> **观测者（observation / observed value）**：通过观测得到，比如数据集中的样本，观测值也可能属于真实值
>
> **预测值（predicted value）**：模型通过**[推理（inference）](https://en.wikipedia.org/wiki/Inference)**所得到的预测信息

所有的机器学习，本质上都是为了减少**人工干预（human intervention）**，使某些任务能够让机器尽可能的自主完成。

其基本思路（或者说普遍任务）是把现实中的问题抽象成数学模型，同时还需要向适合的模型算法提供大量高质量的**训练集（training dataset）**，并给定合适的[**超参数（hyperparameter）**](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning))以指定学习过程的细节，其学习过程称之为**训练（training）**，在训练期间，算法会自动分析出数据中的规律，得出已知输入特征和输出之间的映射关系，并推广到未知的数据，以此构建出**模型**。

模型需要进行**评估（model evaluation）**，即评估模型的**准确度（accuracy）**、**精度（precision）**、**泛化能力**、模型与数据集中的**数据点（data points）**之间的**拟合程度（fitness）**等指标，如果发现存在任何问题或者不满足标准，将会根据反馈对模型进行**优化（model optimization）**，算法会自主调整参数（如权重），并再次训练，机器学习算法将重复**评估与优化**这种**迭代**过程，直到达到精度阈值为止，并最终形成有效的**模型**，过程期间可能会涉及到不同的数据集（如训练集、验证集、测试集）以进行**交叉验证**。

模型在**部署（model deployment）**之后，根据实际应用，可用于**[数据模式识别（pattern recognition）](https://en.wikipedia.org/wiki/Pattern_recognition)**、对未知数据输入**自动生成**或**预测**输出等情景，随着机器学习摄取更多的新数据，算法会不断的得到改进，也变得更为准确。

可见，机器学习的过程与人类学习的过程非常相似，先由别人传授知识，自己从中掌握规律并推广到相似的其它情景中，同时还要对知识的掌握程度进行验证并不断完善。

<img src="/images/post/ml/machine-learning-life-cycle.png" alt="machine-learning-life-cycle" style="zoom: 50%;" />

<center><font size="2">from: <a href="https://www.javatpoint.com/machine-learning-life-cycle">Machine learning Life cycle</a></font></center>

## 训练误差与泛化误差

> [**奥卡姆剃刀原理（Occam's razor）**](https://en.wikipedia.org/wiki/Occam%27s_razor)是一种解决问题的原则，其基本解释为“**如无必要，勿增加实体**”，强调在复杂情况下，优先采用**更简单**的方法（公式或理论等）来解释和解决问题，该原则可更通俗地解释为“**最简单的解释通常是最好的解释**”。
>
> 这个原则也被运用到了机器学习领域中，比如说，在所有可选择的模型中，能够很好地解释已知数据且更为简单的模型，就是最好且更应该被选择的模型。

**训练误差（training error）**指的是模型在**已知**训练集（已知样本）上所表现出的**误差**，这体现了模型与训练集之间的**拟合程度（fitness）**。可通过[**损失函数（loss function）**](https://en.wikipedia.org/wiki/Loss_function)来量化目标的**真实值（基准真相）**与**预测值**之间的差距，损失函数根据模型参数进行定义并取决于数据集，该函数通常是衡量单个数据点（样本）的误差，但也可以用于整个训练集。**成本函数（cost function）**与损失函数类似，不过其通常衡量的是整个训练集，还可能包含一些额外项，如**约束（constraints）**或**惩罚（penalties）**等正则化项。虽然这两种函数并不一样，但在机器学习中通常会将它们视为相同概念，有时它们会被称为**误差函数（error function）**。

**泛化**指的是模型对未知数据的预测能力，[**泛化误差（generalization error）**](https://en.wikipedia.org/wiki/Generalization_error)指的则是模型在对**未知**的新数据进行**预测**时所表现出的**误差**，由于未知数据本应该输出的真实值（基准真相）通常也是未知的，因此这种误差通常无法直接计算，不过可以使用**测试集**来进行评估，因为测试集中的样本是事先准备好的，所以其真实值是已知的。

**拟合程度**、**训练误差**和**泛化误差**是评估机器学习模型性能的重要因素，它们通常都是相关的，需要综合评估从而达到较为平衡的状态，使机器学习模型做出更准确的预测。

当模型对训练集学习得太好时，拟合程度会很高，表现为训练误差很小，但泛化误差可能会比较大，换句话来说就是，模型在训练集上表现良好，而在未知的新数据上表现较差，这种情况为[**过拟合（overfitting）**](https://en.wikipedia.org/wiki/Overfitting)，也称为**过训练（overtraining）**，这主要是因为训练集众多样本中可能存在着**噪音（noise）**，模型复杂度过高且过于依赖训练集，可理解为，模型是在尝试“​记住”训练集，而不是“学习”其中的规律。

反之，当模型对训练集学习得不够好时，将不能很好的拟合训练集，无法准确地捕捉输入与输出变量之间的关系，这种情况就是**欠拟合（underfitting）**，也称为**欠训练（undertraining）**，此时训练误差会较大，这可能是由于模型过于简单、特征提取不足或者正则化过多等因素导致的。

此外，还有一种情况是，模型**不收敛（non-convergence）**，其误差不能趋近于稳定，表现为预测准度飘忽不定，波动较大。

机器学习能够自主从经验中学习来提升自身，可定义一个与模型表现相关的**目标函数（objective function）**用于量化和评估模型的性能（或者说好坏），这是一种比损失函数和成本函数更为宽泛的函数类型，其被设计成是“**可优化**”的，可包含**优化目标**，如何找到最佳参数来求解，使目标函数**最小化（minimize）**或**最大化（maximum）**，就是机器学习中的**[优化问题（optimization problem）](https://en.wikipedia.org/wiki/Mathematical_optimization#Optimization_problems)**。

**目标函数**一般可以是一个合适的**损失函数**，那么最小化损失函数，也就是使训练误差最小化，因此优化的目标就是**减少训练误差**。不过，由于还存在着**泛化误差**这一因素，还得考虑拟合程度，这某种程度体现了模型的泛化能力，过拟合和欠拟合等不良情况都应该要避免，一般不会出现欠拟合的情况，所以实际需要考虑的一般是**过拟合**。

为此，通常需要进行[**正则化（regularization）**](https://en.wikipedia.org/wiki/Regularization_(mathematics))，其可理解为将问题的答案转换为更简单的答案的过程，这符合**奥卡姆剃刀原则**，正则化可以防止**过拟合**，从而减少**泛化误差**，其基本做法是往**目标函数**中添加**正则化项（regularization term）**，如**先验（priors）**、**约束（constraints）**或**惩罚（penalties）**等，它们通常是以**人为设定的超参数**的形式添加入目标函数中，由于添加了额外项，所以目标函数可看作为**损失函数（或成本函数）**加上**正则化项**。

可认为，**优化**本身的目标是为了**减少训练误差**，而**正则化**则是在优化的基础上**防止过拟合**进而**减少泛化误差**，不过，除了正则化外，还有其它一些做法可防止过拟合，比如[**提前停止（early stopping）**](https://en.wikipedia.org/wiki/Early_stopping)、**[剪枝（pruning）](https://en.wikipedia.org/wiki/Decision_tree_pruning)**等。

<img src="/images/post/ml/overfitting-in-machine-learning.png" alt="overfitting-in-machine-learning" style="zoom:67%;" />

<center><font size="2">分类模型的例子，左右两图分别代表欠拟合和过拟合的情况，中间则是较优的拟合状态<br/>假设绿色小球代表小狗、紫色星星代表小猫<br/>from: <a href="https://www.ibm.com/topics/underfitting">What is underfitting?</a></font></center>

<img src="/images/post/ml/Overfitting.svg.png" alt="Overfitting.svg" style="zoom: 80%;" />

<center><font size="2">《过拟合与较优拟合状态重叠起来比较》<br/>绿色线为过拟合、黑色线为较优拟合状态<br/>from: <a href="https://en.wikipedia.org/wiki/Overfitting">Overfitting - Wiki</a></font></center>

## MLOps

**[机器学习运维（MLOps）](https://en.wikipedia.org/wiki/MLOps)**是结合了 **[DevOps](https://en.wikipedia.org/wiki/DevOps)** 和 **GitOps** 原则的一组工作流最佳实践，并演化为一种独立的机器学习生命周期管理方法，其旨在简化机器学习模型的部署和维护过程。

![mlops-cycle](/images/post/ml/mlops-cycle.png)

<center><font size="2">from: <a href="https://www.databricks.com/glossary/mlops">What is MLOps?</a></font></center>

## 深度学习

[**深度学习（DL，deep learning）**](https://en.wikipedia.org/wiki/Deep_learning)为**机器学习**和**神经网络（NN，neural network）**的一个子集，是更为先进的一种机器学习，其基于[**人工神经网络（ANN，artifical neural network）**](https://en.wikipedia.org/wiki/Neural_network_(machine_learning))架构，这是一种从**[神经生物学（neuroscience）](https://en.wikipedia.org/wiki/Neuroscience)**中汲取灵感、受动物大脑中神经网络的结构和功能“启发”的机器学习模型，可理解为对大脑思考和学习等工作方式，以及大脑**[神经元（neuron）](https://en.wikipedia.org/wiki/Neuron)**之间的连接和信息传递的“模拟”，不过当前的神经网络并不是打算模拟生物体的大脑功能（目前也无法做到），所以说是“​启发”比“模拟”更为合适。人工神经网络衍生出了非常多类型，如**卷积神经网络（CNN）**。

深度学习中的神经网络采用的通常都是**多层神经网络（ multi-layered neural networks）**，即由多个**节点层（node layer）**所组成的神经网络，节点指的是[**人工神经元（artificial neuron）**](https://en.wikipedia.org/wiki/Artificial_neuron)，多个人工神经元堆叠成一个节点层，通常会由**输入层（input layer）**、**输出层（output layer）**以及多个**[隐藏层（hidden layer）](https://en.wikipedia.org/wiki/Hidden_layer)**构成一个神经网络，隐藏层是位于输入层和输出层之间的中间层，其通常不直接接受外界信号，也不直接向外界发送信号，所以可看作是一个**[黑匣子（black box）](https://en.wikipedia.org/wiki/Black_box)**，如果像下图这样存在两个或以上的隐藏层，可被视为**深层神经网络**，仅包含三层的只是基本的神经网络。

每个层包含的节点（或人工神经元）可将输入数据转换为信息，供下一层用于特定的**预测**任务，这种多层次的工作方式使深度学习具有深度性，这也是为什么称之为“**深度**”学习的原因，得益于这种结构，机器也可以通过自身的数据处理进行学习。

<img src="/images/post/ml/multi-layered_neural_networks.gif" alt="multi-layered_neural_networks" style="zoom: 80%;" />

<center><font size="2">from: <a href="https://aws.amazon.com/cn/compare/the-difference-between-machine-learning-and-deep-learning/">机器学习与深度学习之间有什么区别？</a></font></center>

从训练集上看，**传统机器学习**一般采用**结构化数据集（structured dataset）**，通常需要先执行特征工程来预处理数据集，即从原始数据中手动选择和提取特征并为其分配权重，而且可能还会为样本分配标签。而**深度学习**一般采用**非结构化数据集（unstructured dataset）**，如图像或音频等，相比于传统机器学习，深度学习只需要**更少的人工干预**即可执行特征工程，其可以从数据中自动习得高级特征，并自行创建新的特征，以区分不同类别的数据，通常还可使用已知错误的反馈进行**自主学习（self-taught learning）**，可将深度学习视为“可拓展的机器学习”。

传统机器学习更适合较为简单且数据量较小的任务，而深度学习的学习能力更强，更适合较为复杂的任务，其通常需要更长的训练时间、更大的数据量，并依赖于更高端的硬件设备，**[语音识别（speech recognition）](https://en.wikipedia.org/wiki/Speech_recognition)**、**[图像识别（image recognition）](https://en.wikipedia.org/wiki/Computer_vision#Recognition)**、**[生成式人工智能（AIGC，AI-generated content）](https://en.wikipedia.org/wiki/Generative_artificial_intelligence)**等都是深度学习的典型案例。

## 涉及知识

以下列举机器学习可能或必定涉及到的一些学科、理论或算法：

- [**线性代数（linear algebra）**](https://en.wikipedia.org/wiki/Linear_algebra)
  - [**标量（scalar）**](https://en.wikipedia.org/wiki/Scalar_(mathematics))
  - [**向量（vector）**](https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics))
  - [**张量（tensor）**](https://en.wikipedia.org/wiki/Tensor)
  - [**矩阵（matrix）**](https://en.wikipedia.org/wiki/Matrix_(mathematics))
- [**微积分（calculus）**](https://en.wikipedia.org/wiki/Calculus)
  - [**傅里叶变换（FT，Fourier transform）**](https://en.wikipedia.org/wiki/Fourier_transform)
- [**概率论（probability theory）**](https://en.wikipedia.org/wiki/Probability_theory)
  - **[贝叶斯定理（Bayes' theorem）](https://en.wikipedia.org/wiki/Bayes%27_theorem)**
- [**统计学（statistics）**](https://en.wikipedia.org/wiki/Statistics)
  - [**回归分析（regression analysis）**](https://en.wikipedia.org/wiki/Regression_analysis)
    - [**线性回归（linear regression）**](https://en.wikipedia.org/wiki/Linear_regression)
    - [**逻辑回归（logistic regression）**](https://en.wikipedia.org/wiki/Logistic_regression)
  - [**分类（classification）**](https://en.wikipedia.org/wiki/Statistical_classification)
  - [**聚类分析（cluster analysis）**](https://en.wikipedia.org/wiki/Cluster_analysis)
  - [**模式识别（pattern recognition）**](https://en.wikipedia.org/wiki/Pattern_recognition)
  - [**生成模型（generative model）**](https://en.wikipedia.org/wiki/Generative_model)
- **[决策论（decision theory）](https://en.wikipedia.org/wiki/Decision_theory)**
  - [**决策树（decision tree）**](https://en.wikipedia.org/wiki/Decision_tree)
  - [**随机决策森林（random decision forests）**](https://en.wikipedia.org/wiki/Random_forest)
- **[逼近理论（approximation theory）](https://en.wikipedia.org/wiki/Approximation_theory)**
- [**优化理论（optimization）**](https://en.wikipedia.org/wiki/Mathematical_optimization)
  - **[凸分析（convex analysis）](https://en.wikipedia.org/wiki/Convex_analysis)**
- [**数据降维（dimensionality reduction）**](https://en.wikipedia.org/wiki/Dimensionality_reduction)
- [**主成分分析（PCA，principal components analysis）**](https://en.wikipedia.org/wiki/Principal_component_analysis)
- **[支持向量机（SVM，support vector machine）](https://en.wikipedia.org/wiki/Support_vector_machine)**
- [**特征提取（feature extraction）**](https://en.wikipedia.org/wiki/Feature_engineering)
- [**特征选择（feature selection）**](https://en.wikipedia.org/wiki/Feature_selection)
- [**元学习（meta learning）**](https://en.wikipedia.org/wiki/Meta-learning_(computer_science))
- [**强化学习（RL，reinforcement learning）**](https://en.wikipedia.org/wiki/Reinforcement_learning)
- **分布式训练（distributed training）**
- **[迁移学习（TL，transfer learning）](https://en.wikipedia.org/wiki/Transfer_learning)**
- [**感知器（perceptron）**](https://en.wikipedia.org/wiki/Perceptron)
- **[计算复杂性理论（computational complexity theory）](https://en.wikipedia.org/wiki/Computational_complexity_theory)**
- [**计算学习理论（computational learning theory）**](https://en.wikipedia.org/wiki/Computational_learning_theory)
- **[计算神经科学（computational neuroscience）](https://en.wikipedia.org/wiki/Computational_neuroscience)**
- **神经网络（NN，neural network）**
  - [**人工神经网络（ANN，artifical neural network）**](https://en.wikipedia.org/wiki/Neural_network_(machine_learning))
  - **深度神经网络（DNN，deep neural network）**
  - [**卷积神经网络（CNN，convolutional neural network）**](https://en.wikipedia.org/wiki/Convolutional_neural_network)
  - [**前馈神经网络（FNN，feedforward neural network）**](https://en.wikipedia.org/wiki/Feedforward_neural_network)
  - [**循环神经网络（RNN，recurrent neural network）**](https://en.wikipedia.org/wiki/Recurrent_neural_network)
    - **[长短期记忆（LSTM，long short-term memory）](https://en.wikipedia.org/wiki/Long_short-term_memory)**
  - [**生成对抗网络（GAN，generative adversarial network）**](https://en.wikipedia.org/wiki/Generative_adversarial_network)
  - [**转换器模型（transformer model）**](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture))
- etc.

## 推荐阅读

- [机器学习术语表 - google](https://developers.google.com/machine-learning/glossary?hl=zh-cn)
- [机器学习基础课程 - google](https://developers.google.com/machine-learning/foundational-courses?hl=zh-cn)
- [《动手学深度学习 第二版》](https://zh.d2l.ai/index.html)
- [模型评估与调优](https://machine-learning-from-scratch.readthedocs.io/zh-cn/latest/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E6%A8%A1%E5%9E%8B%E8%B0%83%E4%BC%98.html#)
- [什么是 MLOps？](https://www.redhat.com/zh/topics/ai/what-is-mlops)
- [Deep Learning: finding that perfect fit](https://www.compact.nl/articles/deep-learning-finding-that-perfect-fit/)
- [Prediction of Wave Forces on the Box-Girder Superstructure of the Offshore Bridge with the Influence of Floating Breakwater](https://www.mdpi.com/2077-1312/11/7/1326)