---
title: 数据处理之归一化、标准化与正则化
tags:
  - 数据处理
  - 标准化
  - 正则化
  - 机器学习
date: 2024-10-22 01:22:48
---



## 特征缩放（feature scaling）

在数据处理中，**归一化 / 规范化（normalization）**和[**标准化（standardization / z-score normalization）**](https://en.wikipedia.org/wiki/Feature_scaling#Standardization_(Z-score_Normalization))是对**数据集（dataset）**进行**[特征缩放（feature scaling）](https://en.wikipedia.org/wiki/Feature_scaling)**的两种常见技术，这通常是数据处理非常重要的一环，一般在**数据预处理（data preprocessing）**阶段执行。

可从一个例子来体会这种处理的重要性，假设有一个数据集，其有多个特征值，而每个特征值的原始取值范围差异很大，且**量纲（dimension）**也可能不同，如果不对这些特征进行处理的话，算法可能会偏向于某一个特征，从而影响算法结果的精度和准确度。

归一化和标准化都可看作是对数据集的重新映射，使特征的取值范围更为集中，**收敛（converge）**到同一数量级上，从而加快算法的**[收敛速度（rate of convergence）](https://en.wikipedia.org/wiki/Rate_of_convergence)**、提升结果精确度和可靠性、甚至带来性能的提升。此外，这还可消除不同特征中的量纲影响，使特征**无量纲化（nondimensionalization）**，进而使算法可更方便地一起处理具有不同量纲的特征，这在处理多维度、多类型数据集时尤为关键。归一化或校准化的数据集甚至可以兼容不同的算法、模型、环境参数、应用情景。

在不同的数据集情景和需求下，归一化和标准化的实现方式、含义和应用情景可能不相同，而是否需要对数据集进行归一化或标准化也得视具体的情况而定，不是所有的数据、模型或算法都需要执行这种处理。

### 归一化（normalization）

$$
{\displaystyle x'={\frac {x-{\text{min}}(x)}{{\text{max}}(x)-{\text{min}}(x)}}}
$$

归一化一般也称为[**最小-最大值规范化（min-max normalization）**](https://en.wikipedia.org/wiki/Feature_scaling#Rescaling_(min-max_normalization))，即根据取值范围的极值（最小值和最大值）将特征收敛到 $[0,1]$ 或 $[-1,1]$ 之间，归一化对**[异常值 / 离群值（outlier）](https://en.wikipedia.org/wiki/Outlier)**敏感，所以不太适合于存在异常值的数据样本。归一化一般适用于以下情况：

- 存在多种不同尺度（取值范围或量纲）的特征，且各特征的极值明确
- 数据较为稳定
- 对输出结果的取值范围有明确要求

### 标准化（standardization）

$$
{\displaystyle x'={\frac {x-{\bar {x}}}{\sigma }}}
$$

标准化的处理方式是，先得到整个数据集的**[平均值（mean）](https://en.wikipedia.org/wiki/Mean)**和**[标准差（SD，standard deviation）](https://en.wikipedia.org/wiki/Standard_deviation)**，然后把每个特征值**减去均值**再**除以标准差**从而得到该特征值处理后的值，映射完成后，会得到一个**均值为 $0$、标准差为 $1$** 的数据集，由于标准差是**[方差（variance）](https://en.wikipedia.org/wiki/Variance)**的平方根，因此方差也是 $1$，可见，处理后的数据集是满足**[标准正态分布（standard normal distribution）](https://en.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution)**的，而且取值范围并不像归一化那样局限于某个特定范围，另外，其对异常值不太敏感，尽管也会受到异常值的影响，但是可进行修正。标准化一般适用于以下情况：

- 数据集符合正态分布（或近似），不过就算不符合正态分布也同样可以使用
- 数据不稳定或可能存在噪点
- 特征极值不明确
- 对输出结果取值范围无限制

## 正则化（regularization）

在**机器学习（ML，machine learning）**领域中，**模型（model）**在已知的**训练集（training dataset）**样本上所表现出来的误差被称为**训练误差（training error）**，这体现了模型与训练集之间的**拟合程度（fitness）**，**泛化（generalization）**指的是模型对未知数据的**预测（prediction）**能力，那么模型在未知数据上的误差被称为**泛化误差（generalization error）**。

拟合程度、训练误差和泛化误差三者相关，提升拟合程度，可减少训练误差，但过于拟合的话，模型会过于依赖训练集，而训练集的众多样本中也可能存在着**噪音（noise）**，因此可能会导致泛化误差变大，这种情况被称为[**过拟合（overfitting）**](https://en.wikipedia.org/wiki/Overfitting)。

可用一个与模型相关且$“$可优化$”$的**目标函数（objective function）**来对拟合进行量化，这可以是一个**损失函数（loss function）**，其可量化目标的**真实值（基准真相）**与**预测值**之间的差距，那么通过学习，找到最佳参数来求解，使目标函数最小化或最大化，就可以据此用来对模型进行**优化（optimize）**，即减少训练误差。

[**正则化（regularization）**](https://en.wikipedia.org/wiki/Regularization_(mathematics))是**防止过拟合、进而减少泛化误差**的一种方式。基本做法是以人为设定的[**超参数（hyperparameter）**](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning))的形式，往目标函数中添加一些**正则化项（regularization term）**，从而对目标函数施加**约束（constraints）**或**惩罚（penalties）**等参数，进而避免过拟合。可理解为将问题的答案转换为更简单的答案的过程，这符合[**奥卡姆剃刀原理（Occam's razor）**](https://en.wikipedia.org/wiki/Occam%27s_razor)，除了正则化外，还有其它一些做法可防止过拟合，比如[**提前停止（early stopping）**](https://en.wikipedia.org/wiki/Early_stopping)、**[剪枝（pruning）](https://en.wikipedia.org/wiki/Decision_tree_pruning)**等。

<img src="/images/post/normalization/overfitting-in-machine-learning.png" alt="overfitting-in-machine-learning" style="zoom:67%;" />

<center><font size="2">分类模型的例子，左右两图分别代表欠拟合和过拟合的情况，中间则是较优的拟合状态</br>假设绿色小球代表小狗、紫色星星代表小猫</br>from: <a href="https://www.ibm.com/topics/underfitting">What is underfitting?</a></font></center>

<img src="/images/post/normalization/Overfitting.svg.png" alt="Overfitting.svg" style="zoom: 80%;" />

<center><font size="2">《过拟合与较优拟合状态重叠起来比较》</br>绿色线为过拟合、黑色线为较优拟合状态</br>from: <a href="https://en.wikipedia.org/wiki/Overfitting">Overfitting - Wiki</a></font></center>

存在很多实现正则化的方式，不过这里仅了解下有哪些常见的方法，而并不关心它们具体是怎么实现的，如：

- **L1 正则化（L1 regularization）**和 **L2 正则化（L2 regularization）**，这是被广泛应用的两种基本正则化方法，它们都是在目标函数中添加**惩罚项**来阻止复杂，它们又可各自称为 [**Lasso（Least absolute shrinkage and selection operator，最小绝对收缩和选择算子）**](https://en.wikipedia.org/wiki/Lasso_(statistics))和[**岭回归（ridge regression）**](https://en.wikipedia.org/wiki/Ridge_regression)，即各自施加了 L1 和 L2 正则化项的线性回归
- [**弹性网络正则化（elastic net regularization）**](https://en.wikipedia.org/wiki/Elastic_net_regularization)
- **[dropout](https://en.wikipedia.org/wiki/Dilution_(neural_networks))**
- **max-norm regularization**
- **谱正则化（spectral norm regularization）**
- **自正交性正则化（self-orthogonality regularization）**
- etc.